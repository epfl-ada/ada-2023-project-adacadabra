{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is destinated to find the collaborators IDs on each video.\n",
    "\n",
    "\n",
    "The organization of the notebook: \n",
    "1) First taks: functions that will help us find the collaborators IDs, and the dataset needed to find the IDS.\n",
    "2) Second task: Importing the dataset and applying our functions to it.\n",
    "3) Third task: Create a new dataframe with collaborators IDs per week (for a join with our other dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) ***FIRST PART: FUNCTIONS:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset we will use in subsequest functions to find the ID\n",
    "df_channels_path = '/Users/selima/Downloads/df_channels_en.tsv'\n",
    "df_channels = pd.read_csv(df_channels_path, sep ='\\t')\n",
    "#keeping only the channel_id and channel_name\n",
    "channel_ids_and_names = df_channels[['channel' , 'name_cc']].copy()\n",
    "channel_ids_and_names.rename(columns={\"channel\": \"channel_id\", \"name_cc\": \"channel_name\"} , inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Function 1: Scrap the youtube links in the description**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will see if there is any of this pattern of links in the description\n",
    "\n",
    "identified channel links types:\n",
    "1) youtube.com/channel/youtubecreators\n",
    "2) youtube.com/@youtubecreators\n",
    "3) youtube.com/c/YouTubeCreators\n",
    "4) youtube.com/user/YouTube\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that for URL 1) and 4) we can find the channel ID wheras in URL 2) and 3) we will find the channel names\n",
    "\n",
    "In our function, if the link refears to the name of a channel, we append it to a new column in our dataframe called 'channel_name', and if it refears an ID we append it to another one 'channel_id'.\n",
    "\n",
    "If there is more than a channel mentionned, we take all the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all youtube channel IDs and channel names\n",
    "#4.4 seconds to run\n",
    "def extract_youtube_channels(text):\n",
    "    # Regular expressions for finding YouTube channel URLs\n",
    "    channel_id_pattern = r'https?://www\\.youtube\\.com/(?:channel/|user/)([\\w-]+)'\n",
    "    channel_name_pattern = r'https?://www\\.youtube\\.com/(?:@|c/)([\\w-]+)'\n",
    "\n",
    "    # Find all links that match the patterns\n",
    "    channel_ids = re.findall(channel_id_pattern, text)\n",
    "    channel_names = re.findall(channel_name_pattern, text)\n",
    "\n",
    "    # Join the IDs and names found (or np.nan if not found)\n",
    "    channel_id = ', '.join(channel_ids) if channel_ids else pd.NA\n",
    "    channel_name = ', '.join(channel_names) if channel_names else pd.NA\n",
    "\n",
    "    return channel_id, channel_name\n",
    "\n",
    "# apply the function and create two new columns\n",
    "#data2[['mentionned_channel_ID', 'mentionned_channel_name']] = data2['description'].apply(\n",
    "    #lambda x: pd.Series(extract_youtube_channels(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping only the channel names and IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Function 2: Filter the IDs obtained. To have a valid ID a) it needs to be an existing ID in our channel_ids_and_names dataset 2) it needs to different from the ID of the creator of the video.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to unique ids to set for faster loop\n",
    "unique_ids_set = set(channel_ids_and_names.channel_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ids_by_existance_and_not_self_mentioning(df, unique_ids_set):\n",
    "    # Define a vectorized function\n",
    "    def process_entry(mentioned_ids, channel_id):\n",
    "        if pd.isna(mentioned_ids):\n",
    "            return pd.NA\n",
    "        ids = [id.strip() for id in mentioned_ids.split(',')]\n",
    "        valid_ids = [id for id in ids if id in unique_ids_set and id != channel_id]\n",
    "        return ','.join(valid_ids) if valid_ids else pd.NA\n",
    "\n",
    "    # Split, process, and rejoin without using apply row-wise\n",
    "    filtered = [process_entry(m_id, c_id) for m_id, c_id in zip(df['mentionned_channel_ID'], df['channel_id'])]\n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Function 3: get the channel IDs from channel names we have**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to a) check that the names are valid (exist in our dataframe) b) add the IDs corresponding to valid names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for faster lookup\n",
    "channel_name_to_id = pd.Series(channel_ids_and_names.channel_id.values, index=channel_ids_and_names.channel_name.str.lower()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_ids_from_valid_names(mentionned_named, channel_name_to_id):\n",
    "    def process_entry(entry):\n",
    "        # Handle missing values\n",
    "        if pd.isna(entry):\n",
    "            return pd.NA\n",
    "        names = str(entry).split(',')\n",
    "        # Lookup each name in the channel_name_to_id dictionary\n",
    "        valid_ids = [channel_name_to_id.get(name.strip().lower()) for name in names if name.strip().lower() in channel_name_to_id]\n",
    "        # Filter out None values in case a name wasn't found in the dictionary\n",
    "        valid_ids = [id for id in valid_ids if id is not None]\n",
    "        return ','.join(valid_ids) if valid_ids else pd.NA\n",
    "\n",
    "    # list comprehension instead of apply for potentially better performance\n",
    "    return [process_entry(entry) for entry in mentionned_named]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now : check that the id we got isn't 1) of the video creator , 2) doesn't appear in filtered_mentionned_channel_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) **Function 4: Check the validity of the IDs we got**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks done a) ids_from_valid_names are not the same as the channel_id b) the id of the channel name doesn't appear in our filtered_mentionned_channel_ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_ids(row):\n",
    "    # split the ids into a list, remove any whitespace, and filter out any empty strings\n",
    "    valid_ids = list(filter(None, [x.strip() for x in str(row['ids_from_valid_names']).split(',')]))\n",
    "    mentioned_ids = set(filter(None, [x.strip() for x in str(row['filtered_mentionned_channel_ID']).split(',')]))\n",
    "    \n",
    "    # remove the channel_id from the valid_ids if it's present\n",
    "    valid_ids = [id for id in valid_ids if id != row['channel_id']]\n",
    "    \n",
    "    # remove any id from valid_ids if it's already present in mentioned_ids\n",
    "    valid_ids = [id for id in valid_ids if id not in mentioned_ids]\n",
    "    \n",
    "    # join the valid ids back into a string\n",
    "    return ','.join(valid_ids) if valid_ids else pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) **Function 5: concatenate both columns of IDs we got to have the final colaborator IDs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_columns(row):\n",
    "    val1 = row['filtered_mentionned_channel_ID']\n",
    "    val2 = row['clean_ids_from_valid_names']\n",
    "    \n",
    "    # Check for both np.nan and '<NA>' string\n",
    "    if pd.isna(val1) and (pd.isna(val2)):\n",
    "        return pd.NA\n",
    "    elif pd.isna(val1):\n",
    "        return val2\n",
    "    elif pd.isna(val2):\n",
    "        return val1\n",
    "    else:\n",
    "        return f\"{val1},{val2}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) **Function 6: group all our functions into one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_chunk(chunk):\n",
    "    # work on a copy to avoid the error\n",
    "    chunk = chunk.copy()\n",
    "    \n",
    "    #function 1\n",
    "    chunk[['mentionned_channel_ID', 'mentionned_channel_name']] = chunk['description'].apply(\n",
    "        lambda x: pd.Series(extract_youtube_channels(x)))\n",
    "    \n",
    "    # drop these columns as we don't need them, work on a less heavy dataset\n",
    "    chunk.drop(['tags', 'description', 'title'], axis=1, inplace=True)\n",
    "    \n",
    "    #function 2\n",
    "    chunk['filtered_mentionned_channel_ID'] = filter_ids_by_existance_and_not_self_mentioning(\n",
    "        chunk[['mentionned_channel_ID', 'channel_id']], unique_ids_set)\n",
    "    \n",
    "    #function 3\n",
    "    chunk['ids_from_valid_names'] = get_channel_ids_from_valid_names(chunk['mentionned_channel_name'], channel_name_to_id)\n",
    "    \n",
    "    #function 4\n",
    "    chunk['clean_ids_from_valid_names'] = chunk.apply(remove_redundant_ids, axis=1)\n",
    "\n",
    "    #function 5\n",
    "    chunk['final_colaborator_ids'] = chunk.apply(concatenate_columns, axis=1)\n",
    "    \n",
    "    #dropping the columns we don't need anymore\n",
    "    preprocessed_chunk = chunk.drop(['mentionned_channel_ID', 'mentionned_channel_name',\n",
    "                                     'filtered_mentionned_channel_ID', 'ids_from_valid_names',\n",
    "                                     'clean_ids_from_valid_names'], axis=1)\n",
    "    \n",
    "    \n",
    "    return preprocessed_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) ***SECOND PART: importing the dataset and applying the preprocessing steps***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty list to store the processed chunks\n",
    "final_dataframe_list = []\n",
    "\n",
    "# read the dataset in chunks and preprocess each chunk\n",
    "with gzip.open(data_path, 'rt', encoding='utf-8') as file:\n",
    "    for chunk in pd.read_json(file, lines=True, chunksize=chunk_size, convert_dates=['upload_date']):\n",
    "        # filter the chunk\n",
    "        filtered_chunk = chunk[\n",
    "            (chunk['categories'] == 'Gaming') & \n",
    "            (chunk['upload_date'] > pd.Timestamp('2016-01-01'))\n",
    "        ]\n",
    "        # preprocess the filtered chunk\n",
    "        preprocessed_chunk = preprocessing_chunk(filtered_chunk)\n",
    "        # append the preprocessed chunk to the list\n",
    "        final_dataframe_list.append(preprocessed_chunk)\n",
    "\n",
    "# concatenate all the preprocessed chunks into a single DataFrame\n",
    "final_dataframe = pd.concat(final_dataframe_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have sufficient data for collaborations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categories                     0\n",
       "channel_id                     0\n",
       "crawl_date                     0\n",
       "dislike_count              81608\n",
       "display_id                     0\n",
       "duration                       0\n",
       "like_count                 81608\n",
       "upload_date                    0\n",
       "view_count                     1\n",
       "final_colaborator_ids    8881049\n",
       "dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3.11 percent of rows where the collaborator ID is not NaN. This represents 285228 out of 9166277 rows.\n"
     ]
    }
   ],
   "source": [
    "nb_not_nan_collaborators = final_dataframe.final_colaborator_ids.dropna().count() / final_dataframe.shape[0]\n",
    "print('We have %0.2f percent of rows where the collaborator ID is not NaN. This represents %i out of %i rows.' % (nb_not_nan_collaborators * 100, final_dataframe.final_colaborator_ids.dropna().count(), final_dataframe.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's export our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe.to_csv('/Users/selima/Desktop/dataframe_collaborations_videos.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THIRD PART: Create a dataframe for collaborators name per week**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create another dataframe, which will contain 3 columns: 1) the channel id of the video creator 2) the week of the year the video was created 3) collaborator ids for this week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to concatenate collaborator IDs, handling NaN values\n",
    "def concatenate_ids(series):\n",
    "    # check if all values are NaN, if so, return NaN\n",
    "    if (series.isna().all()) :\n",
    "        return np.nan # represent missing data\n",
    "    # otherwise, concatenate non-NaN values\n",
    "    return ','.join(series.dropna().astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column Year-Week to our final_dataframe\n",
    "final_dataframe['Year-Week'] = final_dataframe['upload_date'].dt.strftime('%Y-%U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Year-Week' and 'channel_id', and concatenate the final_collaborator_ids\n",
    "grouped = final_dataframe.groupby(['Year-Week', 'channel_id'])\n",
    "\n",
    "# creating a new dataframe with the the new column\n",
    "collaborators_per_week = grouped.agg({\n",
    "    'final_colaborator_ids': concatenate_ids\n",
    "}).reset_index()\n",
    "\n",
    "# rename the new column\n",
    "collaborators_per_week.rename(columns={'final_colaborator_ids': 'collaborators_in_week'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_empty_with_nan(series):\n",
    "    # Replace empty strings with NaN\n",
    "    return series.replace('', pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborators_per_week['collaborators_in_week'] = replace_empty_with_nan(collaborators_per_week['collaborators_in_week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_ids(series):\n",
    "    # Convert all missing value indicators to pd.NA\n",
    "    series = series.replace({np.nan: pd.NA})\n",
    "    \n",
    "    # Check if all values are pd.NA, if so, return pd.NA\n",
    "    if series.isna().all():\n",
    "        return pd.NA\n",
    "    else:\n",
    "        # Concatenate non-missing values\n",
    "        return ','.join(series.dropna().astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40                             UCuAUR88qBEk1rNnk4XD7L1A,<NA>\n",
       "149        UCXC3bB5hQzpRT1fFS0vcWcQ,UCNjdFawISR3ybZcye9tn...\n",
       "186                            UCuGzr7zMBxGcvBCNhl409sg,<NA>\n",
       "232        UCp9AkWp4jfrEyZKhOTo5rbA,UClw5UTugvHO-VL7n-Iax...\n",
       "243                            UCKFtM4-dl0mDXwmfZlkSWoQ,<NA>\n",
       "                                 ...                        \n",
       "1872707    UCOmy8wuTpC95lefU5d1dt2Q,<NA>,UCOmy8wuTpC95lef...\n",
       "1872797                        UCTjMoP3OH9WJA3Td4jd7xCA,<NA>\n",
       "1872804    UCv23U-Dk9kg65Ykmz2wDnpQ,<NA>,UCv23U-Dk9kg65Yk...\n",
       "1872808    UCKhtonFjPN0GJKkJW6iFXng,<NA>,UCKhtonFjPN0GJKk...\n",
       "1872813    UCzW9IT5ylJ87DLcorSpM2Eg,UC4wUSUO1aZ_NyibCqIjp...\n",
       "Name: collaborators_in_week, Length: 101109, dtype: object"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collaborators_per_week.collaborators_in_week.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborators_per_week.to_csv('/Users/selima/Desktop/collaborators_per_week.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
